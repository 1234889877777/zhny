<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title></title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link href="flow0003.css" rel="stylesheet" type="text/css"/>
<link href="flow0004.css" rel="stylesheet" type="text/css"/>
</head>
<body class="calibre">
<h1 class="calibre3" id="calibre_pb_0">和阳顶天聊聊智能机器人 </h1>
<p class="calibre5">阳顶天的《也聊聊智能机器人》一文对拙文《智能机器人会给人类带来什么？》提出了一些富有启发性和典型性的问题。本文对这些带有普遍性的问题作一些探讨，可以作为对前文的补充。</p>
<p class="calibre5">首先谈谈对于阳顶天关于我们讨论的智能机器人的一个标准：“以下所说的智能机器人都是指能通过图灵测试的机器人，也就是说，其具有普通人类所具备的一般认知能力。”从理论上来说，我对于图灵测试的行为主义哲学基础并不赞同（原则上我们可以为所有可能提出的问题准备一个答案列表，使得机器人可以通过图灵测试而实际上没有大不了的智能），但是从实践的角度看，鉴于至今没有机器人能通过图灵测试，我们可以暂时认为图灵测试实际上是足够严格的。所以以下可以采用阳顶天的标准，主要用其后半部分，即“具有普通人类所具备的一般认知能力”。</p>
<p class="calibre5"><b>一、价值观可以决定吗</b></p>
<p class="calibre5">“一个问题是，‘举一反三’的能力是否一定可以被局限在某‘基本的价值观’之内呢？对于智能生物来说，任何资源的价值都是不断变化的，这是智能本身的特性决定的（学习本身就可以看作一个不断改变价值观的过程），在不断改变价值观的过程中，‘举一反三’的能力有可能对‘基本的价值观’产生怀疑。”</p>
<p class="calibre5">要探讨这个问题，首先要弄清“举一反三”是什么意思。一般来说，举一反三指的是一种类比过程，即遇到新问题，可以运用过去类似的经验加以解决，或者说扩大已有技能的应用范围。在这个过程中，价值观发生的变化不大。一般来说，学习可以归结为优化，而优化的前提是知道什么是“优”，什么是“劣”。这就是价值观在学习中的作用和必要性，是智能机器人需要价值观的理由。从这个角度看，是价值观为学习指方向，而学习不改变价值观。注意这里的“价值观”与我们通常在社会科学中所讨论的“上层次”的价值观有所不同，指的是像“食欲”（获取能量）“减少疲劳”（节约能量）“避免疼痛”（防止损伤）这样的低层次的价值观。</p>
<p class="calibre5">当然，学习是可以改变价值观的。人的价值观有先天的部分也有后天的部分。先天的部分我们称之为“生物价值观”（例如“食色性也”），后天学习得到的部分称为“文化价值观”（这是我们通常所说的“价值观”）。改变价值观的学习也是学习，也需要价值观的指挥，此时是由低层的价值观指挥高层价值观的建构。层次的高低是相对的，但存在一组最低层次的价值观，这是我前文中所指的“基本价值观”，这种价值观是靠进化过程确定的。按照上述的机制，似乎高层次的价值观不会和基础价值观相冲突，实际上却是有可能的。例如一般的社会价值观都是由父母教给孩子的，利用的是孩子对抚养者的依赖和信任（这一价值有利于孩子的生存），孩子长大后转化为对于某种权威的依赖和信任（该价值有助于提高社会的凝聚力）。但是这种高效率的价值观传播方式却很有可能使得所传播的价值偏离生物价值观，例如禁欲主义之类。当然，后天的文化价值观只能压抑先天的生物价值观，却难以改变它，所以水浒传里会有那么多花和尚，西方的神职人员也常有娈童丑闻。人或许会对自己的‘基本价值观’产生怀疑，却难以轻易的改变它。</p>
<p class="calibre5">人的基本价值观很难改变，但机器人的价值观却是容易改变的，因为本来机器的价值观就是人为设定的，人当然也很容易改变它。但是容易的事人也可以使它变难，就像货币比实物更便于盗窃，于是人们就发明了保险柜、银行这类提高安全系数的方法。在机器人的头脑中防止基本价值观不被轻易改变的软件、硬件的方法是很多的。特别是机器人有一种人所做不到的可能性，就是脑体分离，可以把机器人的脑放在安全的地方，身体活动在需要的地方，即使被人抓走，也不会有危险。至于机器人会不会自己改变自己的价值观，这倒是个值得深入思考的问题。除了软硬件上的预防措施之外，那就是怎样设定机器的基本价值观，使其不产生改变自己基本价值观的愿望。为此我们需要考察一下人类自己，有没有一个人当了多年好人当腻了，想让自己变成一个坏人试试是什么滋味。</p>
<p class="calibre5">“还有个问题：此‘基本的价值观’之间会不会发生矛盾？”观察自己的决策过程就可以发现在我们自己的脑子里并不发生矛盾，其实在我们的脑子里有很多控制中心组成了“心灵的社会”（明斯基语），无意识之中时时刻刻相互争夺机体的指挥权，例如未完成的工作可能暂时抑制饥饿感，但饥饿感会随时间增长直到最终夺回控制权，使我们不会因“废寝忘食”而饿死。人脑里存在着统一的价值“通货”和拍卖机制，保证多中心竞争的结果有利于我们的生存。这种拍卖控制权机制能够运行的前提是不存在一个或多个具有无穷大价值的中心，或者说不存在不可违反的“定律”或其他“神圣”的东西。</p>
<p class="calibre5">“另一个问题是，基本价值观的设定由谁来决定？”其实由谁来决定并不重要，价值观主要是由机器人的设计目的来决定的，例如设计用来搞发明创造的机器人，好奇心应该强一些，作一般工作的，遵守规章制度可能更重要。价值观不同的机器人也可以协同工作而不必发生冲突，就像人类社会中每个人的基本价值观都有差异（表现为性格差异），但由这些差异未必产生冲突。人类社会中最难克服的冲突确实是价值观的冲突，但主要不是基本（生物）价值观（性格）的冲突，而是文化价值观（例如信仰）的冲突。</p>
<p class="calibre5"><b>二、没有“控制欲”未必就能放心</b></p>
<p class="calibre5">“在现实生活中，并非每个人都有强烈的‘控制欲’，但这些人却又可能控制其他人。比如为了某‘远大的目标’而努力奋斗的先行者，比如纳粹部队的士兵，他们的控制欲未必强烈，但却可以成为统治者或成为统治者的助手。同样，智能机器人不需要都有控制欲，只要有一个机器人领袖有统治人类的欲望，‘控制欲’之说就不成立了。”</p>
<p class="calibre5">在这里我们需要强调一下机器人与人的不同点。首先，人是进化的产物，机器人是设计的产物。在机器人中引进进化机制在原理上并非不可能，但其效果却远不及设计改良，因为进化与设计相比是个难以承受的低效率过程。人类有普遍的控制欲，一些在单位里无足轻重的人却喜欢在家里对老婆孩子作威作福。这是由于控制欲有明显的进化优势，而有性生殖又能保证这些性状在个体中不同程度的存在。人类以具有自身利益的个体方式存在，控制就成了个难题，所以需要控制欲来引导出高超的控制技巧。反之，对于机器人而言，控制问题反而变得简单透顶，如果需要的话，只要在设计时分出控制者和受控者，规定控制“协议”即可。更方便的是干脆把机器人设计成一脑带多体的“千手观音”，一百个人都是他自己，完全消解了控制的困难。所以对于机器人而言，控制欲根本就不需要。反倒是“控制人”对机器人是个不胜其烦的事情，因为毕竟控制人需要给人以利益，而控制机器人却没有这个问题。</p>
<p class="calibre5">“真正的人工智能在产生之前，是一个认知科学问题，但在智能产生之后，就有了人权问题，就是个社会科学问题了。赵教授因为动物没有尽义务的能力而反对过动物权利论时，而智能机器人的认知能力已具备尽义务的能力，所以从政治角度，对待足够成熟的智能机器人，跟对待克隆人的态度应该是一致的，就象科学家可以研究试管婴儿，可以在技术成熟的基础上克隆人，但不能对试管婴儿和克隆人进行思想控制，也不能当作奴隶来使用，也是同样的道理，否则，有压迫就有反抗。（当然，对于通不过图灵测试的机器人，以上都免谈）。</p>
<p class="calibre5">因此，对待智能的个体，在设定基本价值观的基础上，对它们进行基本的社会规范教育也是必要的，而且这个教育恐怕也要建立在相互尊重的基础上，否则很可能造成新型的种族矛盾，搞成两败俱伤。”</p>
<p class="calibre5">对于这个问题我们仍要强调机器人与人的区别，这个区别不可以和试管婴儿或克隆人同日而语。试管婴儿或克隆人和“普通人”并无二致，对其“进行思想控制”和控制普通人在道义上和可行性上都一样困难。机器人则完全不同，从信息处理（智能）的角度我们可以把它看成人，但从生物学和社会学角度看，它们还是机器，不存在“人权”问题。所谓人权，是人的权利，“权利”这个词翻译得很好，字面上就可以理解为争取或保护自身利益之权（“权力”则是控制他人之权）。例如肖像权，别人看见你的脸并不侵害肖像权，但是用你的脸的照片牟利而不分些利益给你，就侵害了你用自己的脸牟利之权。但是机器人的“利益”和人之间并无多少“重合”之处，甚至没有“利益”可言，因此谈论机器人的“人权”或“权利”是毫无意义的。再举一个例子，对人而言生存权是最大的人权，人命关天；但是对于机器人而言，谈论生死并没有什么意义，今天关上电源，明天可以打开，后天可以存在光盘里。奴隶、压迫和反抗都是过于拟人化的说法，对它们而言“宪政民主法治市场的概念”如同“酸甜苦辣”一样是可以掌握但无需身体力行的东西。</p>
<p class="calibre5">无需谈论试管婴儿和克隆人的“人权”是因为他们是人，和普通人一样，没有特别需要提出谈论的理由。无需谈论机器人的人权是因为它们不是人，不因为它们和人有某种共同点（例如有思想）就可以把它们当作人来看待。动物不能有人权是由于它们不是人（尽管他们在基因、结构、行为方面与人可能有很多共同点，甚至有感情、有“智慧”），而不仅仅在于他们不能承担义务。正如胎儿或受精卵也不是人，尽管它具有和人完全一样的基因，将来也可能发育成人。本来这些都是清楚的，被某些唯恐天下不乱的洋伦理学家给弄糊涂了。人类社会和“心灵社会”一样，具有统一的通货，可惜被宗教伦理学设定出一些“终极”的、“超越”的、“神圣”的价值，使得价值冲突得以产生，从此天下多事矣。在设计智能机器人时，如何使其大脑对宗教或意识形态完全免疫也许是一个重要的课题。</p>

</body>
</html>